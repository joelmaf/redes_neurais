{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joelmaf/redes_neurais/blob/main/ALUNO_etapa_transformacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8K70Mebuy5W"
      },
      "source": [
        "# Problema:  Prever se um cliente vai cancelar sua assinatura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YaS8AiWY2XZ"
      },
      "source": [
        "# Estrutura das variáveis\n",
        "\n",
        "\n",
        "*   **Estado:** O estado de onde o cliente é originário\n",
        "*   **Tempo de conta:** Número de dias que o cliente está usando os serviços\n",
        "*   **Código de área:** A área de onde o cliente é originário\n",
        "*   **Número de telefone:** O número de telefone do cliente\n",
        "*   **Plano internacional:** O status do plano internacional do cliente\n",
        "*   **Plano de correio de voz:** O status do plano de correio de voz do cliente\n",
        "*   **Número de mensagens de correio de voz:** Número de mensagens de correio de voz enviadas pelo cliente\n",
        "*   **Total de minutos diurnos:** Total de minutos de chamadas feitos por um cliente durante o dia\n",
        "*   **Total de chamadas diurnas:** Número total de chamadas feitas por um cliente durante o dia\n",
        "*   **Total de cobranças diurnas:** Valor total cobrado a um cliente **durante** o dia\n",
        "*   **Total de minutos vespertinos:** Total de minutos de chamadas feitos por um cliente durante a tarde\n",
        "*   **Total de chamadas vespertinas:** Número total de chamadas feitas por um cliente durante a tarde\n",
        "*   **Total de cobranças vespertinas:** Valor total cobrado a um cliente durante a tarde\n",
        "*   **Total de minutos noturnos:** Total de minutos de chamadas feitos por um cliente durante a noite\n",
        "*   **Total de chamadas noturnas:** Número total de chamadas feitas por um cliente durante a noite\n",
        "*   **Total de cobranças noturnas:** Valor total cobrado a um cliente durante a noite\n",
        "*   **Total de minutos internacionais:** Total de minutos de chamadas internacionais feitas por um cliente\n",
        "*   **Total de chamadas internacionais:** Número total de chamadas internacionais feitas por um cliente\n",
        "*   **Total de cobranças internacionais:** Valor total cobrado por chamadas internacionais feitas por um cliente\n",
        "*   **Chamadas ao serviço de atendimento ao cliente:** Número total de chamadas feitas ao serviço de atendimento ao cliente\n",
        "*   **Cancelamento:** Cancelado ou não\n",
        "\n",
        "https://github.com/kuldeep1909/Sony-Research-Machine-Learning-Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms46HPyxlXSp"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas\n",
        "#!pip install numpy\n",
        "#!pip install matplotlib\n",
        "#!pip install seaborn\n",
        "#!pip install scikit-learn\n",
        "#!pip install jupyter\n",
        "!pip install shap\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUmWnZ_FYdL8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE4dlmH3lzBf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "url = 'Sony_data.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Divisão em Treino e Teste\n",
        "\n",
        "**Razões para Dividir em Treino e Teste**\n",
        "\n",
        "1. Treinar o Modelo: Durante o treinamento, o modelo de aprendizado de máquina utiliza o conjunto de dados de treino para ajustar seus parâmetros internos (pesos, coeficientes, etc.). O objetivo do treinamento é encontrar padrões nos dados e aprender a fazer previsões. Conjunto de treino: Este conjunto de dados é utilizado para treinar o modelo. Ele contém tanto as variáveis de entrada (features) quanto as saídas (rótulos/valores alvo) que o modelo deve prever.\n",
        "\n",
        "2. Avaliar a Generalização do Modelo: Depois de treinar o modelo, é necessário verificar como ele se comporta com dados que não foram utilizados durante o treinamento. Isso é feito com o conjunto de teste. O conjunto de teste fornece uma avaliação objetiva da capacidade de generalização do modelo, ou seja, sua habilidade de fazer previsões corretas em dados novos e desconhecidos.\n",
        "Sem essa etapa, você não teria como saber se o modelo está simplesmente memorizando os dados de treino (overfitting) ou se está realmente aprendendo os padrões subjacentes que podem ser aplicados a novos dados.\n",
        "\n",
        "3. Evitar Overfitting:\n",
        "Overfitting ocorre quando o modelo se ajusta tão bem aos dados de treino que perde a capacidade de generalizar para novos dados. O modelo pode aprender \"ruídos\" ou padrões específicos dos dados de treino que não existem no mundo real. Ao avaliar o modelo com dados de teste, que ele nunca viu antes, podemos identificar se ele está superajustando os dados de treino ou se é capaz de generalizar para novos exemplos.\n",
        "\n",
        "4. Obter uma Estimativa de Performance Real:\n",
        "Avaliar o modelo com o conjunto de teste fornece uma estimativa realista da sua performance em dados futuros. Isso é importante porque, no mundo real, o modelo será aplicado a dados que não foram vistos durante o treinamento.\n",
        "Métricas como acurácia, precisão, revocação, F1-score e erro quadrático médio calculadas com o conjunto de teste nos dão uma ideia de quão bem o modelo irá performar em produção.\n",
        "\n",
        "\n",
        "**Fluxo de Trabalho**\n",
        "\n",
        "* Treinamento do Modelo: O conjunto de treino é usado para ajustar o modelo, encontrar padrões nos dados e determinar os melhores parâmetros para realizar previsões.\n",
        "\n",
        "* Validação (opcional): Em muitos casos, além da divisão entre treino e teste, um terceiro conjunto, chamado de validação, é utilizado para afinar os hiperparâmetros do modelo (parâmetros que não são ajustados automaticamente, como a profundidade de uma árvore de decisão ou a taxa de aprendizado em redes neurais).\n",
        "\n",
        "* Teste do Modelo: O conjunto de teste, que contém dados completamente novos, é utilizado para testar o modelo e medir sua capacidade de fazer previsões corretas em dados desconhecidos."
      ],
      "metadata": {
        "id": "FReilYr71M5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisão em treino e teste\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Conjunto de treino: \", X_train.shape)\n",
        "print(\"Conjunto de teste: \", X_test.shape)"
      ],
      "metadata": {
        "id": "XfbVG4Eo1OH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processamento dos dados"
      ],
      "metadata": {
        "id": "Mbc0VQXrx7UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "LuTJIpFtx5X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe(include='object')"
      ],
      "metadata": {
        "id": "qQqCfkWLx5hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrigir tipo\n",
        "def correct_type(df_origem):\n",
        "  df_origem['Codigo_area'] = df_origem['Codigo_area'].astype('object')\n",
        "  return df_origem"
      ],
      "metadata": {
        "id": "-Pv2seL9x5kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UYCBdWCXx5nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe(include='object')"
      ],
      "metadata": {
        "id": "EPt-Wn8wx5rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deletar colunas inúteis\n",
        "def delete_column(df_origem):\n",
        "  df_origem = df_origem.drop('Numero_telefone', axis=1)\n",
        "  return df_origem"
      ],
      "metadata": {
        "id": "CnhrXG7Ex5ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = delete_column(X_train)\n",
        "X_test = delete_column(X_test)"
      ],
      "metadata": {
        "id": "LCdphVatyIlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yDwpN-GbyPLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binning\n",
        "\n",
        "Binning é o processo de dividir dados contínuos em intervalos ou \"bins\" (também conhecidos como \"caixas\" ou \"faixas\"). Isso é feito para transformar variáveis contínuas (como idade, salário, número de mensagens, etc.) em variáveis categóricas, facilitando a análise e a interpretação dos dados, especialmente em situações onde estamos interessados em categorias ou classes.\n",
        "\n",
        "Vantagens?\n",
        "* Simplificação: Ajuda a reduzir a complexidade dos dados contínuos transformando-os em categorias discretas.\n",
        "* Melhora a interpretabilidade: Em vez de lidar com números contínuos, você pode trabalhar com categorias mais fáceis de entender, como \"baixo\", \"médio\" ou \"alto\".\n",
        "* Melhor ajuste para alguns algoritmos: Alguns modelos de aprendizado de máquina funcionam melhor com variáveis categóricas, e o binning pode melhorar a performance nesses casos.\n",
        "* Manuseio de outliers: Pode ajudar a reduzir o impacto de outliers ao agrupar dados em bins.\n"
      ],
      "metadata": {
        "id": "MEvdYW7MyPU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribuição original\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.histplot(X_train['Numero_mensagens_voz'], kde=True, color='blue')\n",
        "plt.title('Antes da transformação', fontsize=14)\n",
        "plt.xlabel('Número de mensagens de correio de voz', fontsize=12)\n",
        "plt.ylabel('Frequencia', fontsize=12)"
      ],
      "metadata": {
        "id": "6yjHjmyyyIog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comando pd.cut()\n",
        "\n",
        "O método cut é usado para segmentar e classificar os dados da coluna \".\n",
        "* bins: Define os limites dos bins.\n",
        "* labels: Define os rótulos que serão atribuídos a cada bin.\n",
        "* right: Indica que os limites do intervalo são exclusivos no lado direito, ou seja, por exemplo, se for FALSE um bin 0-21 inclui os valores de 0 até 20, mas não inclui 21."
      ],
      "metadata": {
        "id": "dNw6vlEcyZIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning em dois 2 bins: 0-20, 20+\n",
        "def binning(df_origem, bins,labels, columns):\n",
        "  df_origem[columns] = pd.cut(df_origem[columns], bins=bins, labels=labels, right=False)\n",
        "  return df_origem"
      ],
      "metadata": {
        "id": "bQwYdQ2ByIrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QOfyvsSKyIu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "rYFv_nmSyIyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequência depois do bining\n",
        "sns.countplot(x='Numero_mensagens_voz', data=X_train, palette='Set2')\n",
        "plt.title('Depois Binning (Baixo, Alto)', fontsize=14)\n",
        "plt.xlabel('Binned Número Mensagen Voz', fontsize=12)\n",
        "plt.ylabel('Frequência', fontsize=12)"
      ],
      "metadata": {
        "id": "uMy7GJMfyI1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning em 3 bins: 0-2, 3-4, 4+\n",
        "bins = [0, 3, 5, np.inf]\n",
        "labels = ['Baixo', 'Médio', 'Alto']\n",
        "\n",
        "X_train = binning(X_train, bins,labels,'Chamadas_atendimento')\n",
        "X_test = binning(X_test, bins,labels,'Chamadas_atendimento')"
      ],
      "metadata": {
        "id": "ybUWSpuGx5xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "wte-SLpSx50J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Log Transformation\n",
        "\n",
        "Log Transformation (Transformação Logarítmica) é uma técnica usada para transformar variáveis de dados, aplicando a função logarítmica a elas. Essa transformação é muito útil em situações onde os dados são altamente assimétricos (ou seja, possuem skewness), ou quando existem outliers que podem distorcer os resultados de análises ou modelagens.\n",
        "\n",
        "Objetivo da Log Transformation:\n",
        "* Reduzir Skewness: Muitas variáveis, especialmente em domínios como finanças e economia, têm distribuições altamente assimétricas (skewed), onde a maior parte dos dados se concentra em uma extremidade e há uma cauda longa. A transformação logarítmica pode \"achatar\" a distribuição, tornando-a mais simétrica e mais parecida com uma distribuição normal, o que é importante para muitas análises estatísticas.\n",
        "* Reduzir a influência de outliers: Em dados altamente dispersos, valores extremos (outliers) podem ter um grande impacto. A transformação logarítmica reduz o impacto desses valores extremos, fazendo com que eles se ajustem melhor ao restante dos dados.\n",
        "* Normalização: Em alguns casos, aplicar a transformação logarítmica pode aproximar os dados de uma distribuição normal, o que é uma suposição comum em várias técnicas de análise de dados e aprendizado de máquina."
      ],
      "metadata": {
        "id": "oA1I-AULypcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(X_train['Total_chamadas_internacionais'], bins=10, kde=True, color='blue')\n",
        "plt.title('Distribuição original')\n",
        "plt.xlabel('Total Chamadas Internacionais')\n",
        "plt.ylabel('Frequência', fontsize=12)"
      ],
      "metadata": {
        "id": "OuLN4dKXx53b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### np.log1p()\n",
        "\n",
        "O np.log1p() é uma função da biblioteca NumPy que calcula o logaritmo natural (base e) de um número mais 1. Em outras palavras, np.log1p(x) é equivalente a log(1 + x). Isso é feito para evitar o problema de calcular log(0), que não é definido (o logaritmo de zero não existe). Ao adicionar 1 aos valores, garantimos que a transformação logarítmica possa ser aplicada mesmo quando os dados contêm zeros"
      ],
      "metadata": {
        "id": "2QSTJi6ayu9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando log transformation para reduzir skewness (adicionando 1 para evitar log(0))\n",
        "def log_transformation(df_origem):\n",
        "  df_origem['Total_chamadas_internacionais'] = np.log1p(df_origem['Total_chamadas_internacionais'])\n",
        "  return df_origem"
      ],
      "metadata": {
        "id": "o3CLxhWcx56S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = log_transformation(X_train)\n",
        "X_test = log_transformation(X_test)"
      ],
      "metadata": {
        "id": "8yqXIuC9x59X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "3BRGbbPRyyLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(X_train['Total_chamadas_internacionais'], bins=10, kde=True, color='green')\n",
        "plt.title('Distribuição transformada (Log Transformation)')\n",
        "plt.xlabel('Total chamadas internacionais transformadas')\n",
        "plt.ylabel('Frequência', fontsize=12)"
      ],
      "metadata": {
        "id": "2nTeETwYyyPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding\n",
        "\n",
        "Encoding (ou codificação) é o processo de converter dados categóricos (geralmente texto ou rótulos) em uma representação numérica que pode ser entendida e processada por algoritmos de aprendizado de máquina. Muitos algoritmos de aprendizado de máquina e modelos estatísticos não conseguem trabalhar diretamente com variáveis categóricas, como textos ou rótulos, pois eles precisam de entradas numéricas. Exemplo de dados categóricos: Colunas com valores como 'sim', 'não', 'alto', 'médio', 'baixo', ou categorias como 'vermelho', 'azul', 'verde'.\n",
        "\n",
        "A codificação é importante porque a maioria dos modelos de aprendizado de máquina, como regressões lineares, árvores de decisão e redes neurais, só conseguem processar entradas numéricas. Portanto, antes de aplicar qualquer modelo a um conjunto de dados que contenha variáveis categóricas, é necessário converter esses dados para uma forma que o modelo consiga interpretar. Se não fizermos a codificação, o modelo simplesmente não será capaz de processar os dados ou fornecerá resultados incorretos.\n",
        "\n",
        "Principais Métodos de Encoding:\n",
        "* Label Encoding (Codificação de Rótulos): Transforma cada categoria em um número inteiro único. Útil quando existe uma relação ordinal entre as categorias (como \"baixo\", \"médio\" e \"alto\"). No entanto, não deve ser usado para dados categóricos sem ordem, pois o modelo pode interpretar as relações numéricas incorretamente. Exemplo: Suponha que temos uma coluna com as categorias [\"Alto\", \"Médio\", \"Baixo\"]. Com o Label Encoding, essas categorias seriam convertidas em [0, 1, 2], ou seja, \"Baixo\" -> 0, \"Médio\" -> 1, \"Alto\" -> 2.\n",
        "\n",
        "* One-Hot Encoding: Converte categorias em colunas binárias (0 ou 1), onde cada coluna representa uma categoria. Ideal para dados categóricos nominais (sem ordem). A codificação One-Hot evita a criação de uma relação ordinal artificial entre categorias. Exemplo: Se a coluna original contém [\"Vermelho\", \"Verde\", \"Azul\"], o One-Hot Encoding transformaria essa coluna em três colunas: Vermelho, Verde e Azul, onde cada linha teria 1 na coluna correspondente à sua categoria e 0 nas demais.\n",
        "\n",
        "* Target Encoding (Codificação por Alvo): Substitui as categorias por valores calculados com base na média da variável-alvo (ou outra estatística) para cada categoria. Útil quando a cardinalidade (número de categorias) é alta e existe uma correlação entre as categorias e a variável-alvo. Exemplo: Se estamos tentando prever se um cliente vai cancelar uma assinatura (0 ou 1) e temos uma coluna \"País\", podemos calcular a média de cancelamento para cada país e substituir o nome do país pela média.\n",
        "\n",
        "* Binary Encoding: Combina Label Encoding e One-Hot Encoding. Converte as categorias em números inteiros (Label Encoding) e, em seguida, codifica esses números inteiros em binário. Funciona bem quando há muitas categorias, evitando a criação de muitas colunas extras como no One-Hot Encoding.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U32qL8Rsy5i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "JRT8qo_LyySN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.select_dtypes(include=['object','category']).columns"
      ],
      "metadata": {
        "id": "FJBeYIXwyyVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pCauCnwHx6AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = one_hot_encoder(X_train)\n",
        "X_test_copy = one_hot_encoder(X_test)\n",
        "X_train_copy.head(5)"
      ],
      "metadata": {
        "id": "9j6dizbV-UVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando mapping: encoding da variável booleana Cancelamento\n",
        "def mapping_target(df_origem):\n",
        "    if isinstance(df_origem, pd.DataFrame):\n",
        "        boolean_cols = df_origem.select_dtypes(include=['bool']).columns\n",
        "        df_origem[boolean_cols] = df_origem[boolean_cols].applymap(lambda x: 1 if x else 0)\n",
        "        return df_origem\n",
        "    elif isinstance(df_origem, pd.Series) and df_origem.dtype == 'bool':\n",
        "        return df_origem.map({True: 1, False: 0})"
      ],
      "metadata": {
        "id": "Vr4iSl0VzAiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = mapping_target(X_train_copy)\n",
        "X_test_copy = mapping_target(X_test_copy)\n",
        "\n",
        "y_train = mapping_target(y_train)\n",
        "y_test = mapping_target(y_test)\n",
        "\n",
        "X_train_copy.head(5)"
      ],
      "metadata": {
        "id": "BipFa2fL-ZF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "LdAD4O6AzAmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['Plano_internacional'].unique()"
      ],
      "metadata": {
        "id": "W4y8wDRczApR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['Plano_correio'].unique()"
      ],
      "metadata": {
        "id": "NCV67nG7zAsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['Numero_mensagens_voz'].unique()"
      ],
      "metadata": {
        "id": "JFMN39vtzI5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['Chamadas_atendimento'].unique()"
      ],
      "metadata": {
        "id": "_qQbfCBVzI9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando Label Encoder\n",
        "def label_encoder(df_origem):\n",
        "  le = LabelEncoder()\n",
        "  df_origem['Plano_internacional'] = le.fit_transform(df_origem['Plano_internacional'])\n",
        "  df_origem['Plano_correio'] = le.fit_transform(df_origem['Plano_correio'])\n",
        "  df_origem['Chamadas_atendimento'] = le.fit_transform(df_origem['Chamadas_atendimento'])\n",
        "  df_origem['Numero_mensagens_voz'] = le.fit_transform(df_origem['Numero_mensagens_voz'])\n",
        "  return df_origem"
      ],
      "metadata": {
        "id": "X_Jz6Ys_zJDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = label_encoder(X_train)\n",
        "X_test = label_encoder(X_test)"
      ],
      "metadata": {
        "id": "GSpsNztv46JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "2KQu2gkrzJGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['Codigo_area'] = X_train['Codigo_area'].astype(int)\n",
        "X_test['Codigo_area'] = X_test['Codigo_area'].astype(int)\n",
        "X_train['Codigo_area'].unique()"
      ],
      "metadata": {
        "id": "73CgOMYlzJJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando Target Encoding\n",
        "def target_encoding(X, y, column1, column2):\n",
        "    X_temp = X.copy()\n",
        "    X_temp['Cancelamento'] = y\n",
        "\n",
        "    mean_cancelamento = X_temp.groupby(column1)['Cancelamento'].mean()\n",
        "    X[column2] = X[column1].map(mean_cancelamento)\n",
        "    X.drop(columns=[column1], inplace=True)\n",
        "    return X"
      ],
      "metadata": {
        "id": "pYkm8QJGzJL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = target_encoding(X_train, y_train,'Estado','Media_cancelamento_por_estado')\n",
        "X_train = target_encoding(X_train, y_train,'Codigo_area','Media_cancelamento_por_codigo_area')\n",
        "\n",
        "X_test = target_encoding(X_test, y_test, 'Estado','Media_cancelamento_por_estado')\n",
        "X_test = target_encoding(X_test, y_test, 'Codigo_area','Media_cancelamento_por_codigo_area')\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "AVM8aatO2rT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature engineering\n",
        "\n",
        "A feature engineering (engenharia de características) envolve, por exemplo, a criação de novas variáveis ou atributos derivados dos dados originais. O objetivo é fornecer informações adicionais que possam ajudar o modelo a capturar padrões e fazer previsões mais precisas.\n",
        "\n",
        "Criar novas colunas pode aumentar a expressividade dos dados:\n",
        "* Novas colunas podem trazer informações derivadas que não estão explícitas nas colunas originais.\n",
        "* Melhorar o desempenho do modelo: A criação de novas colunas pode ajudar o modelo a encontrar padrões ou relacionamentos entre variáveis que não seriam evidentes apenas com os dados brutos. Essas variáveis derivadas podem capturar melhor a dinâmica do problema, resultando em um modelo mais preciso.\n",
        "* Em muitos casos, as variáveis originais podem não ser suficientes para prever adequadamente a variável-alvo. Criar novas features pode complementar os dados e enriquecer a informação disponível para o modelo.\n",
        "* Capturar relações não lineares: Em vez de trabalhar diretamente com valores absolutos, as relações entre variáveis podem ser mais úteis, mais informativas e permitir que o modelo capture melhor os padrões de consumo do cliente.\n",
        "* Reduzir a dimensionalidade: Em alguns casos, criar novas colunas pode permitir que você reduza a complexidade dos dados. Ao criar colunas que combinam informações, você pode eliminar a necessidade de várias colunas originais, reduzindo assim a dimensionalidade e tornando o treinamento do modelo mais eficiente.\n",
        "* Normalizar informações: Algumas colunas, podem variar significativamente, mas a relação entre variáveis pode ser mais consistente e relevante para o modelo. Isso ajuda a capturar o comportamento de forma normalizada.\n",
        "* Identificar características importantes: Através da criação de novas colunas, pode-se gerar variáveis que sejam mais diretamente relacionadas à variável-alvo. Por exemplo, se estamos tentando prever se um cliente vai cancelar sua assinatura, a eficiência do uso do plano (como custo por minuto) pode ser um preditor mais eficaz do que as cobranças totais ou os minutos usados isoladamente.\n"
      ],
      "metadata": {
        "id": "vy0cPkVTzr4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando novas colunas\n",
        "def feature_engineering(df_origem):\n",
        "  df_origem['Total_chamadas_diurnas_por_minuto'] = df_origem['Total_cobrancas_diurnas'] / df_origem['Total_minutos_diurnos']\n",
        "  df_origem['Total_chamadas_vespertinas_por_minuto'] = df_origem['Total_cobrancas_vespertinas'] / df_origem['Total_minutos_vespertinos']\n",
        "  df_origem['Total_chamadas_noturnas_por_minuto'] = df_origem['Total_cobrancas_noturnas'] / df_origem['Total_minutos_noturnos']\n",
        "  df_origem['Total_chamadas_internacionais_por_minuto'] = df_origem['Total_cobrancas_internacionais'] / df_origem['Total_minutos_internacionais']\n",
        "  return df_origem"
      ],
      "metadata": {
        "id": "Fdr9mqIyzAvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = feature_engineering(X_train)\n",
        "X_test = feature_engineering(X_test)\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "nwnaYnuL25zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature selection\n",
        "\n",
        "Feature Selection (ou Seleção de Variáveis ou Seleção de Atributos) é o processo de identificar e selecionar um subconjunto das variáveis ou features mais relevantes em um conjunto de dados que são mais importantes para o desempenho de um modelo de aprendizado de máquina. O objetivo da seleção de variáveis é melhorar a performance do modelo, reduzindo a complexidade, eliminando ruídos, e aumentando a interpretabilidade.\n",
        "\n",
        "\n",
        "* Redução de Overfitting: Ao reduzir o número de variáveis irrelevantes ou redundantes, você diminui o risco de overfitting (quando o modelo se ajusta demais aos dados de treino e perde a capacidade de generalizar para dados novos).\n",
        "* Aumento da Performance do Modelo: Eliminar variáveis desnecessárias pode melhorar a performance do modelo, tanto em termos de precisão quanto de velocidade. Modelos mais simples tendem a ser mais rápidos para treinar e predizer, além de serem menos propensos a erro.\n",
        "* Redução de Tempo de Treinamento: Menos variáveis significa menos dados para processar, o que reduz o tempo de treino do modelo e a quantidade de recursos computacionais necessários.\n",
        "* Melhoria da Interpretabilidade: Modelos com menos variáveis são mais fáceis de entender e interpretar.\n",
        "* Remoção de Ruído: Variáveis irrelevantes ou redundantes podem adicionar ruído aos dados, dificultando a identificação de padrões e impactando negativamente o desempenho do modelo.\n"
      ],
      "metadata": {
        "id": "j77k64yf0BS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "sopwVgNM3FxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_correlation_matrix = pd.concat([X_train, y_train], axis=1).corr(method='pearson')\n",
        "mask = np.triu(np.ones_like(pearson_correlation_matrix, dtype=bool))\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(pearson_correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, annot_kws={\"fontsize\": 10}, mask=mask)\n",
        "plt.title('Matriz de Correlação de Pearson')"
      ],
      "metadata": {
        "id": "Y7mt4VgAzrAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recuperar as features mais importantes\n",
        "rf_model = RandomForestClassifier(n_estimators=500)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "importance = rf_model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train.columns,'Importancia': importance}).sort_values(by='Importancia', ascending=False)\n",
        "\n",
        "# Plotting feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importancia', y='Feature', data=feature_importance_df, palette='viridis')\n",
        "plt.title('Importãncia da Feature na Predição de Cancelamento')\n",
        "plt.xlabel('Medida de Importância')\n",
        "plt.ylabel('Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "70jZXp4jzrDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a variância de cada coluna (ou variável) em X_train.\n",
        "# A variância mede a dispersão dos dados em torno da média.\n",
        "# Quanto maior a variância, mais dispersos são os valores em uma variável.\n",
        "\n",
        "variance = X_train.var()\n",
        "variance.sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "TLfdJo0yzrGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "bestfeatures = SelectKBest(k=10)\n",
        "\n",
        "fit = bestfeatures.fit(X_train,y_train)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X_train.columns)\n",
        "\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "print(featureScores.nlargest(10,'Score'))"
      ],
      "metadata": {
        "id": "ui9ZR5J139kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retirando algumas colunas\n",
        "columns_to_drop = ['Total_cobrancas_diurnas', 'Total_cobrancas_noturnas', 'Total_cobrancas_vespertinas',\n",
        "                   'Total_cobrancas_internacionais', 'Numero_mensagens_voz', 'Total_chamadas_diurnas_por_minuto',\n",
        "                   'Total_chamadas_vespertinas_por_minuto', 'Total_chamadas_internacionais_por_minuto']\n",
        "\n",
        "X_train.drop(columns=columns_to_drop, inplace=True)\n",
        "X_test.drop(columns=columns_to_drop, inplace=True)\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "bmf3UF1zzrJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Balanceando a classe"
      ],
      "metadata": {
        "id": "bS1aYcjV0UkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Cancelamento', data=pd.concat([X_train, y_train], axis=1), palette='Set2')\n",
        "plt.title('Distribuição da Classe antes de ADASYN')\n",
        "plt.xlabel('Cancelamento')\n",
        "plt.ylabel('Contagem')\n",
        "plt.xticks([0, 1], labels=['Não Cancelado', 'Cancelado'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2hzdtDCEzrMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando o balanceamento de classes\n",
        "adasyn = ADASYN(sampling_strategy='minority', random_state=42)\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "WMYGY3ui0Rqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Cancelamento', data=pd.concat([X_resampled, y_resampled], axis=1), palette='Set2')\n",
        "plt.title('Distribuição da Classe após ADASYN')\n",
        "plt.xlabel('Cancelamento')\n",
        "plt.ylabel('Contagem')\n",
        "plt.xticks([0, 1], labels=['Não Cancelado', 'Cancelado'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hFq_dCzG0RuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before_count = pd.Series(y_train).value_counts()\n",
        "after_count = pd.Series(y_resampled).value_counts()\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Antes': before_count,\n",
        "   'Depois': after_count\n",
        "})\n",
        "\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "iWkThCGN0Rxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train = X_resampled\n",
        "#y_train = y_resampled"
      ],
      "metadata": {
        "id": "BCjBQmLE4b7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalização\n",
        "\n",
        "Normalização é uma técnica de pré-processamento de dados que ajusta a escala dos valores de uma variável para que todos os valores estejam dentro de um intervalo específico, geralmente entre 0 e 1. O objetivo da normalização é garantir que diferentes variáveis tenham a mesma escala, o que pode ser definitivo para o desempenho de algoritmos de aprendizado de máquina.\n",
        "\n",
        "A normalização é especialmente útil quando as variáveis do conjunto de dados têm diferentes unidades ou escalas (por exemplo, uma variável representando salários em milhares e outra representando idades). Sem normalização, os algoritmos de aprendizado de máquina podem atribuir mais importância a variáveis com maiores valores numéricos, mesmo que essas variáveis não sejam necessariamente mais importantes.\n",
        "\n",
        "* Normalização: Escala os dados para que fiquem em um intervalo específico, geralmente entre 0 e 1. Normalização é útil quando você quer manter os valores dentro de um intervalo limitado, como entre 0 e 1.\n",
        "\n",
        "Fórmula comum:\n",
        "$$\n",
        "X_{\\text{norm}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "$$\n",
        "\n",
        "\n",
        "* Padronização (Z-score): Transforma os dados para que tenham média 0 e desvio padrão 1. Ela remove a média e escala de acordo com a variabilidade dos dados. Padronização é útil quando você quer manter a distribuição original dos dados, mas precisa alinhar a média e o desvio padrão.\n",
        "\n",
        "Fórmula comum:\n",
        "\n",
        "$$\n",
        "X_{\\text{padronizado}} = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZCLi2ua4LL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As funções RobustScaler, StandardScaler, FunctionTransformer e MinMaxScaler da biblioteca Scikit-learn são usadas para transformar os dados de diferentes maneiras, dependendo do tipo de pré-processamento que você precisa realizar.\n",
        "\n",
        "1. RobustScaler: Transforma os dados removendo a mediana e escalando os valores com base no intervalo interquartil (IQR). Ou seja, é robusto em relação a outliers, pois usa a mediana e os percentis 25 e 75 para o escalonamento.\n",
        "\n",
        "$$\n",
        "X_{\\text{robusto}} = \\frac{X - Q2}{Q3 - Q1}\n",
        "$$\n",
        "\n",
        "\n",
        "2. StandardScaler: Transforma os dados para que tenham média 0 e desvio padrão 1. Remove a média e divide pelos desvios padrão de cada feature (variável).\n",
        "\n",
        "3. MinMaxScaler: Escala os dados para um intervalo específico, normalmente entre 0 e 1, mas você pode definir outros intervalos, como [-1, 1].\n",
        "\n",
        "4. FunctionTransformer: Permite aplicar funções arbitrárias aos dados para transformá-los. Ele permite que você defina uma transformação personalizada sem precisar criar um objeto separado para isso. Isso pode ser útil quando você deseja aplicar transformações específicas, como transformar logaritmos ou outras operações."
      ],
      "metadata": {
        "id": "0YKX6xlo4S1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SOg2XdI94ZHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = scaling(X_train)\n",
        "X_test = scaling(X_test)"
      ],
      "metadata": {
        "id": "S1Mts9006wnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_csv('X_train.csv', index=False)\n",
        "y_train.to_csv('y_train.csv', index=False)\n",
        "X_test.to_csv('X_test.csv', index=False)\n",
        "y_test.to_csv('y_test.csv', index=False)"
      ],
      "metadata": {
        "id": "Owci7Ty50R0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "am-zMzQt4iqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "0dVzEb8k6rsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO6m6HcEP5AHudqdVCY17d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}